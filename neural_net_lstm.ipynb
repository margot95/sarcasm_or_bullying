{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9158f4",
   "metadata": {},
   "source": [
    "## Neural network (RNN with LSTM layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d5eb92",
   "metadata": {},
   "source": [
    "Why a neural network with LSTM layer?\n",
    "\n",
    "We are still on our binary classification of tweets/comments, the label 0 is for \"sarcasm\" and the label 1 means \"cyberbullying\".\n",
    "\n",
    "A neural network, unlike the previous Naive Bayes model that we have tried out, will make use of the context (as opposed to dealing with individual words).\n",
    "And a Long Short Term Memory layer means the model can handle “long-term dependencies” (which a classical RNN cannot handle).\n",
    "LSTM supposedly also solves the problem of vanishing gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1705d4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 12:36:36.333538: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-02 12:36:36.443208: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-02 12:36:36.447280: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-02 12:36:36.447292: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-02 12:36:36.470027: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-02 12:36:37.080075: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-02 12:36:37.080220: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-02 12:36:37.080228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16e116d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('merged_all_modified_csv.csv')\n",
    "X = data['comment_cleaned_lower']\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44dde6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29751,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4723984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=124, vector_size=30, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "#Train a word2vec model on our sentences\n",
    "word2vec = Word2Vec(sentences=[str(x) for x in X_train], vector_size=30, window =2, min_count=5)\n",
    "#word2vec = Word2Vec(sentences=str(X_train), vector_size=30, window =2, min_count=5)\n",
    "print(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a8a2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = [] \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)    \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, [str(x) for x in X_train])\n",
    "X_test_embed = embedding(word2vec, [str(x) for x in X_test])\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c57920f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29751, 200, 30), (12751, 200, 30))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape, X_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b60f742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking our X_train_pad and X_test_pad, they should be np arrays, 3-dim,  \n",
    "#last dimension must be of the size of the word2vec embedding space, and 1st dim must be of size of X_train and X_test\n",
    "\n",
    "for X in [X_train_pad, X_test_pad]:\n",
    "    assert type(X) == np.ndarray\n",
    "    assert X.shape[-1] == word2vec.wv.vector_size\n",
    "\n",
    "\n",
    "assert X_train_pad.shape[0] == len(X_train)\n",
    "assert X_test_pad.shape[0] == len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd04fa1",
   "metadata": {},
   "source": [
    "## Baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa565252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7490168397700918"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_accuracy = y_train.sum() / y_train.shape[0]\n",
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626d539",
   "metadata": {},
   "source": [
    "## RNN model, without transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e8cafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = word2vec.wv.vectors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e7e0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking())\n",
    "    model.add(layers.LSTM(20, activation=\"tanh\"))\n",
    "    model.add(layers.Dense(10, activation = 'relu'))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f0c0754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "233/233 [==============================] - 87s 351ms/step - loss: 0.5372 - accuracy: 0.7471\n",
      "Epoch 2/10\n",
      "233/233 [==============================] - 83s 357ms/step - loss: 0.4463 - accuracy: 0.7887\n",
      "Epoch 3/10\n",
      "233/233 [==============================] - 87s 372ms/step - loss: 0.4370 - accuracy: 0.8163\n",
      "Epoch 4/10\n",
      "233/233 [==============================] - 73s 315ms/step - loss: 0.3956 - accuracy: 0.8379\n",
      "Epoch 5/10\n",
      "233/233 [==============================] - 77s 330ms/step - loss: 0.3519 - accuracy: 0.8533\n",
      "Epoch 6/10\n",
      "233/233 [==============================] - 77s 331ms/step - loss: 0.3398 - accuracy: 0.8659\n",
      "Epoch 7/10\n",
      "233/233 [==============================] - 32s 138ms/step - loss: 0.3158 - accuracy: 0.8687\n",
      "Epoch 8/10\n",
      "233/233 [==============================] - 38s 165ms/step - loss: 0.3033 - accuracy: 0.8749\n",
      "Epoch 9/10\n",
      "233/233 [==============================] - 74s 316ms/step - loss: 0.3222 - accuracy: 0.8707\n",
      "Epoch 10/10\n",
      "233/233 [==============================] - 73s 314ms/step - loss: 0.2953 - accuracy: 0.8814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc5c1433760>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback = EarlyStopping(monitor='accuracy', patience=3)\n",
    "model.fit(X_train_pad, y_train, epochs=10, batch_size=128, callbacks=[callback], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc59daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_1 (Masking)         (None, 200, 30)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 20)                4080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,301\n",
      "Trainable params: 4,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4748c1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 25s 59ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.97180146],\n",
       "       [0.95489734],\n",
       "       [0.9362625 ],\n",
       "       ...,\n",
       "       [0.9579762 ],\n",
       "       [0.9467767 ],\n",
       "       [0.9647433 ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0bfe5",
   "metadata": {},
   "source": [
    "## Same model, this time pretrained on much larger (and similar) dataset : Glove\n",
    "transfer learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bfe6c1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "#print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f5c38ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_transfer = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f578554",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size_transfer = word2vec_transfer.vector_size\n",
    "vocab_size_transfer = word2vec_transfer.vectors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00539072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []  \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)   \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_transfer = embedding(word2vec_transfer, [str(x) for x in X_train])\n",
    "X_test_embed_transfer = embedding(word2vec_transfer, [str(x) for x in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f699bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding\n",
    "X_train_pad_transfer = pad_sequences(X_train_embed_transfer, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad_transfer = pad_sequences(X_test_embed_transfer, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "156c9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transfer = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49811497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "233/233 [==============================] - 81s 332ms/step - loss: 0.5343 - accuracy: 0.7447\n",
      "Epoch 2/100\n",
      "233/233 [==============================] - 75s 324ms/step - loss: 0.4814 - accuracy: 0.7645\n",
      "Epoch 3/100\n",
      "233/233 [==============================] - 77s 332ms/step - loss: 0.4202 - accuracy: 0.8061\n",
      "Epoch 4/100\n",
      "233/233 [==============================] - 76s 327ms/step - loss: 0.3711 - accuracy: 0.8325\n",
      "Epoch 5/100\n",
      "233/233 [==============================] - 76s 325ms/step - loss: 0.3329 - accuracy: 0.8525\n",
      "Epoch 6/100\n",
      "233/233 [==============================] - 76s 325ms/step - loss: 0.3081 - accuracy: 0.8633\n",
      "Epoch 7/100\n",
      "233/233 [==============================] - 84s 360ms/step - loss: 0.2894 - accuracy: 0.8726\n",
      "Epoch 8/100\n",
      "233/233 [==============================] - 76s 326ms/step - loss: 0.2756 - accuracy: 0.8803\n",
      "Epoch 9/100\n",
      "233/233 [==============================] - 76s 325ms/step - loss: 0.2609 - accuracy: 0.8879\n",
      "Epoch 10/100\n",
      "233/233 [==============================] - 76s 326ms/step - loss: 0.2480 - accuracy: 0.8948\n",
      "Epoch 11/100\n",
      "233/233 [==============================] - 76s 326ms/step - loss: 0.2367 - accuracy: 0.9001\n",
      "Epoch 12/100\n",
      "233/233 [==============================] - 79s 339ms/step - loss: 0.2298 - accuracy: 0.9026\n",
      "Epoch 13/100\n",
      "233/233 [==============================] - 79s 338ms/step - loss: 0.2210 - accuracy: 0.9079\n",
      "Epoch 14/100\n",
      "233/233 [==============================] - 80s 342ms/step - loss: 0.2115 - accuracy: 0.9107\n",
      "Epoch 15/100\n",
      "233/233 [==============================] - 86s 367ms/step - loss: 0.2039 - accuracy: 0.9157\n",
      "Epoch 16/100\n",
      "233/233 [==============================] - 79s 341ms/step - loss: 0.1997 - accuracy: 0.9163\n",
      "Epoch 17/100\n",
      "233/233 [==============================] - 81s 349ms/step - loss: 0.1912 - accuracy: 0.9216\n",
      "Epoch 18/100\n",
      "233/233 [==============================] - 87s 372ms/step - loss: 0.1850 - accuracy: 0.9241\n",
      "Epoch 19/100\n",
      "233/233 [==============================] - 98s 421ms/step - loss: 0.1786 - accuracy: 0.9275\n",
      "Epoch 20/100\n",
      "233/233 [==============================] - 81s 349ms/step - loss: 0.1738 - accuracy: 0.9283\n",
      "Epoch 21/100\n",
      "233/233 [==============================] - 84s 362ms/step - loss: 0.1676 - accuracy: 0.9314\n",
      "Epoch 22/100\n",
      "233/233 [==============================] - 76s 326ms/step - loss: 0.1640 - accuracy: 0.9336\n",
      "Epoch 23/100\n",
      "233/233 [==============================] - 78s 333ms/step - loss: 0.1600 - accuracy: 0.9354\n",
      "Epoch 24/100\n",
      "233/233 [==============================] - 76s 325ms/step - loss: 0.1558 - accuracy: 0.9374\n",
      "Epoch 25/100\n",
      "233/233 [==============================] - 75s 324ms/step - loss: 0.1517 - accuracy: 0.9397\n",
      "Epoch 26/100\n",
      "233/233 [==============================] - 75s 323ms/step - loss: 0.1479 - accuracy: 0.9407\n",
      "Epoch 27/100\n",
      "233/233 [==============================] - 95s 408ms/step - loss: 0.1434 - accuracy: 0.9447\n",
      "Epoch 28/100\n",
      "233/233 [==============================] - 99s 423ms/step - loss: 0.1398 - accuracy: 0.9452\n",
      "Epoch 29/100\n",
      "233/233 [==============================] - 91s 389ms/step - loss: 0.1372 - accuracy: 0.9456\n",
      "Epoch 30/100\n",
      "233/233 [==============================] - 92s 393ms/step - loss: 0.1345 - accuracy: 0.9473\n",
      "Epoch 31/100\n",
      "233/233 [==============================] - 94s 404ms/step - loss: 0.1270 - accuracy: 0.9519\n",
      "Epoch 32/100\n",
      "233/233 [==============================] - 76s 327ms/step - loss: 0.1272 - accuracy: 0.9506\n",
      "Epoch 33/100\n",
      "233/233 [==============================] - 76s 327ms/step - loss: 0.1246 - accuracy: 0.9517\n",
      "Epoch 34/100\n",
      "233/233 [==============================] - 77s 330ms/step - loss: 0.1205 - accuracy: 0.9538\n",
      "Epoch 35/100\n",
      "233/233 [==============================] - 77s 330ms/step - loss: 0.1175 - accuracy: 0.9564\n",
      "Epoch 36/100\n",
      "233/233 [==============================] - 78s 335ms/step - loss: 0.1156 - accuracy: 0.9564\n",
      "Epoch 37/100\n",
      "233/233 [==============================] - 75s 324ms/step - loss: 0.1132 - accuracy: 0.9580\n",
      "Epoch 38/100\n",
      "233/233 [==============================] - 76s 325ms/step - loss: 0.1106 - accuracy: 0.9580\n",
      "Epoch 39/100\n",
      "233/233 [==============================] - 77s 329ms/step - loss: 0.1091 - accuracy: 0.9591\n",
      "Epoch 40/100\n",
      "233/233 [==============================] - 79s 339ms/step - loss: 0.1082 - accuracy: 0.9596\n",
      "Epoch 41/100\n",
      "233/233 [==============================] - 76s 326ms/step - loss: 0.1055 - accuracy: 0.9607\n",
      "Epoch 42/100\n",
      "233/233 [==============================] - 76s 328ms/step - loss: 0.1045 - accuracy: 0.9613\n",
      "Epoch 43/100\n",
      "233/233 [==============================] - 78s 333ms/step - loss: 0.1016 - accuracy: 0.9631\n",
      "Epoch 44/100\n",
      "233/233 [==============================] - 76s 328ms/step - loss: 0.1003 - accuracy: 0.9635\n",
      "Epoch 45/100\n",
      "233/233 [==============================] - 77s 329ms/step - loss: 0.0976 - accuracy: 0.9649\n",
      "Epoch 46/100\n",
      "233/233 [==============================] - 76s 327ms/step - loss: 0.0970 - accuracy: 0.9641\n",
      "Epoch 47/100\n",
      "233/233 [==============================] - 78s 334ms/step - loss: 0.0955 - accuracy: 0.9661\n",
      "Epoch 48/100\n",
      "233/233 [==============================] - 77s 329ms/step - loss: 0.0931 - accuracy: 0.9671\n",
      "Epoch 49/100\n",
      "233/233 [==============================] - 77s 329ms/step - loss: 0.0933 - accuracy: 0.9666\n",
      "Epoch 50/100\n",
      "233/233 [==============================] - 77s 330ms/step - loss: 0.0897 - accuracy: 0.9675\n",
      "Epoch 51/100\n",
      "233/233 [==============================] - 79s 340ms/step - loss: 0.0887 - accuracy: 0.9682\n",
      "Epoch 52/100\n",
      "233/233 [==============================] - 92s 395ms/step - loss: 0.0882 - accuracy: 0.9679\n",
      "Epoch 53/100\n",
      "233/233 [==============================] - 81s 347ms/step - loss: 0.0875 - accuracy: 0.9683\n",
      "Epoch 54/100\n",
      "233/233 [==============================] - 76s 327ms/step - loss: 0.0859 - accuracy: 0.9696\n",
      "Epoch 55/100\n",
      "233/233 [==============================] - 77s 331ms/step - loss: 0.0857 - accuracy: 0.9696\n",
      "Epoch 56/100\n",
      "233/233 [==============================] - 85s 366ms/step - loss: 0.0842 - accuracy: 0.9695\n",
      "Epoch 57/100\n",
      "233/233 [==============================] - 89s 383ms/step - loss: 0.0828 - accuracy: 0.9701\n",
      "Epoch 58/100\n",
      "233/233 [==============================] - 76s 328ms/step - loss: 0.0820 - accuracy: 0.9707\n",
      "Epoch 59/100\n",
      "233/233 [==============================] - 77s 332ms/step - loss: 0.0803 - accuracy: 0.9714\n",
      "Epoch 60/100\n",
      "233/233 [==============================] - 77s 329ms/step - loss: 0.0790 - accuracy: 0.9713\n",
      "Epoch 61/100\n",
      "233/233 [==============================] - 77s 332ms/step - loss: 0.0773 - accuracy: 0.9726\n",
      "Epoch 62/100\n",
      "233/233 [==============================] - 76s 326ms/step - loss: 0.0771 - accuracy: 0.9730\n",
      "Epoch 63/100\n",
      "233/233 [==============================] - 78s 333ms/step - loss: 0.0760 - accuracy: 0.9734\n",
      "Epoch 64/100\n",
      "233/233 [==============================] - 79s 340ms/step - loss: 0.0759 - accuracy: 0.9731\n",
      "Epoch 65/100\n",
      "233/233 [==============================] - 84s 360ms/step - loss: 0.0747 - accuracy: 0.9741\n",
      "Epoch 66/100\n",
      "233/233 [==============================] - 78s 334ms/step - loss: 0.0735 - accuracy: 0.9733\n",
      "Epoch 67/100\n",
      "233/233 [==============================] - 77s 332ms/step - loss: 0.0711 - accuracy: 0.9744\n",
      "Epoch 68/100\n",
      "233/233 [==============================] - 84s 361ms/step - loss: 0.0724 - accuracy: 0.9745\n",
      "Epoch 69/100\n",
      "233/233 [==============================] - 79s 338ms/step - loss: 0.0711 - accuracy: 0.9738\n",
      "Epoch 70/100\n",
      "233/233 [==============================] - 76s 328ms/step - loss: 0.0694 - accuracy: 0.9754\n",
      "Epoch 71/100\n",
      "233/233 [==============================] - 76s 328ms/step - loss: 0.0685 - accuracy: 0.9752\n",
      "Epoch 72/100\n",
      "233/233 [==============================] - 76s 328ms/step - loss: 0.0691 - accuracy: 0.9759\n",
      "Epoch 73/100\n",
      "233/233 [==============================] - 78s 333ms/step - loss: 0.0690 - accuracy: 0.9756\n",
      "Epoch 74/100\n",
      "233/233 [==============================] - 76s 325ms/step - loss: 0.0662 - accuracy: 0.9757\n",
      "Epoch 75/100\n",
      "233/233 [==============================] - 79s 340ms/step - loss: 0.0666 - accuracy: 0.9765\n",
      "Epoch 76/100\n",
      "233/233 [==============================] - 73s 313ms/step - loss: 0.0646 - accuracy: 0.9768\n",
      "Epoch 77/100\n",
      "233/233 [==============================] - 75s 324ms/step - loss: 0.0644 - accuracy: 0.9766\n",
      "Epoch 78/100\n",
      "233/233 [==============================] - 77s 329ms/step - loss: 0.0623 - accuracy: 0.9779\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 76s 325ms/step - loss: 0.0639 - accuracy: 0.9774\n",
      "Epoch 80/100\n",
      "233/233 [==============================] - 76s 326ms/step - loss: 0.0628 - accuracy: 0.9778\n",
      "Epoch 81/100\n",
      "233/233 [==============================] - 76s 328ms/step - loss: 0.0614 - accuracy: 0.9782\n",
      "Epoch 82/100\n",
      "233/233 [==============================] - 76s 325ms/step - loss: 0.0614 - accuracy: 0.9786\n",
      "Epoch 83/100\n",
      "233/233 [==============================] - 77s 329ms/step - loss: 0.0609 - accuracy: 0.9793\n",
      "Epoch 84/100\n",
      "233/233 [==============================] - 77s 329ms/step - loss: 0.0591 - accuracy: 0.9793\n",
      "Epoch 85/100\n",
      "233/233 [==============================] - 75s 321ms/step - loss: 0.0596 - accuracy: 0.9787\n",
      "Epoch 86/100\n",
      "233/233 [==============================] - 79s 338ms/step - loss: 0.0588 - accuracy: 0.9792\n",
      "Epoch 87/100\n",
      "233/233 [==============================] - 83s 356ms/step - loss: 0.0579 - accuracy: 0.9796\n",
      "Epoch 88/100\n",
      "233/233 [==============================] - 84s 360ms/step - loss: 0.0580 - accuracy: 0.9797\n",
      "Epoch 89/100\n",
      "233/233 [==============================] - 73s 315ms/step - loss: 0.0566 - accuracy: 0.9804\n",
      "Epoch 90/100\n",
      "233/233 [==============================] - 76s 325ms/step - loss: 0.0567 - accuracy: 0.9809\n",
      "Epoch 91/100\n",
      "233/233 [==============================] - 76s 328ms/step - loss: 0.0563 - accuracy: 0.9806\n",
      "Epoch 92/100\n",
      "233/233 [==============================] - 76s 327ms/step - loss: 0.0544 - accuracy: 0.9802\n",
      "Epoch 93/100\n",
      "233/233 [==============================] - 76s 326ms/step - loss: 0.0543 - accuracy: 0.9804\n",
      "Epoch 94/100\n",
      "233/233 [==============================] - 76s 328ms/step - loss: 0.0538 - accuracy: 0.9809\n",
      "Epoch 95/100\n",
      "233/233 [==============================] - 78s 334ms/step - loss: 0.0537 - accuracy: 0.9817\n",
      "Epoch 96/100\n",
      "233/233 [==============================] - 76s 326ms/step - loss: 0.0525 - accuracy: 0.9814\n",
      "Epoch 97/100\n",
      "233/233 [==============================] - 76s 327ms/step - loss: 0.0522 - accuracy: 0.9823\n",
      "Epoch 98/100\n",
      "233/233 [==============================] - 79s 341ms/step - loss: 0.0514 - accuracy: 0.9818\n",
      "Epoch 99/100\n",
      "233/233 [==============================] - 79s 339ms/step - loss: 0.0511 - accuracy: 0.9824\n",
      "Epoch 100/100\n",
      "233/233 [==============================] - 76s 326ms/step - loss: 0.0506 - accuracy: 0.9829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc594da4910>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transfer.fit(X_train_pad_transfer, y_train, epochs=100, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a92cdfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 35s 62ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.9960357e-01],\n",
       "       [9.9983239e-01],\n",
       "       [9.9295008e-01],\n",
       "       ...,\n",
       "       [9.9990988e-01],\n",
       "       [4.3896434e-04],\n",
       "       [9.9986994e-01]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transfer.predict(X_test_pad_transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73e9affc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 - 22s - loss: 0.1285 - accuracy: 0.9602 - 22s/epoch - 56ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model_transfer.evaluate(X_test_pad_transfer, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951c998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
