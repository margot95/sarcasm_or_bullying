{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46614dd",
   "metadata": {},
   "source": [
    "# BERT transformer for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5dda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b6ac0",
   "metadata": {},
   "source": [
    "## Fetch the data, constitute our X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61da2966",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('merged_all_modified_csv.csv')\n",
    "#see naives_bayes notebook to see this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a365970",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df['comment_cleaned_lower']\n",
    "y = merged_df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af775232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29751,), (12751,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d190d3",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f8a62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 16:07:43.841395: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-02 16:07:43.975755: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-02 16:07:44.010053: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-02 16:07:44.690901: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-02 16:07:44.691043: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-02 16:07:44.691050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LABEL_1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(X_train[42000], return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f230965",
   "metadata": {},
   "source": [
    "## Bert for sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3db5dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df=  merged_df[:40000]\n",
    "eval_df = merged_df[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24853f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TransformerModel\n",
    "#model = ClassificationModel('roberta', 'roberta-base', use_cuda=False)\n",
    "\n",
    "# Train the model\n",
    "#model.train_model(merged_df[['comment_cleaned_lower', 'label']][:3000])\n",
    "\n",
    "# Evaluate the model\n",
    "#result, model_outputs, wrong_predictions = model.eval_model(eval_df[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e6ac4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see huggingface.com\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\", problem_type=\"multi_label_classification\")\n",
    "\n",
    "inputs = tokenizer([str(x) for x in X_train[:1]], return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
    "\n",
    "# To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
    "num_labels = len(model.config.id2label)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-yelp-polarity\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "labels = torch.sum(\n",
    "    torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
    ").to(torch.float)\n",
    "loss = model(**inputs, labels=labels).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bae8999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b4254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b29fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
